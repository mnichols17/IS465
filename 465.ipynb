{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "465.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPL4UJsnb7RjyxVhndPkbX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnichols17/IS465/blob/master/465.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5hULCTwYNyc",
        "colab_type": "text"
      },
      "source": [
        "This section of code imports the necessary packages and declares some initial variables. It also includes a function that prints the 10 most relevant words when searching for a specific topic. The function takes in the words and prints them according to how it's supposed to be viewed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Pw4IVunFXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20\n",
        "\n",
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XYXlDZZd-_",
        "colab_type": "text"
      },
      "source": [
        "This section combs through the dataset and strips it of non-relevant words. It save the cleand up data into a new variable called data_samples. It also tells the reader how long this operation took."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-eI2UjZc7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True)\n",
        "data_samples = data[:n_samples]\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULWrtvc9bkp3",
        "colab_type": "text"
      },
      "source": [
        "This section extracts tf-idf features for the NMF. The data samples are then modified with the features and saved into a new variable: tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V16LRHScSvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "t0 = time()\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRukgS2AasVa",
        "colab_type": "text"
      },
      "source": [
        " This section extracts tf features for LDA. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaSXizccap56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlzteEWXcavO",
        "colab_type": "text"
      },
      "source": [
        " These two blocks of code fit the NMF model with the proper features related to th type of NMF model it is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAAaVkCXcaJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
        "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0yu8MWhaIg",
        "colab_type": "text"
      },
      "source": [
        "This code prints all the relevant words from the 10 topics according to the NMF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq82a9onhcER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3-mi_gXm5tg",
        "colab_type": "text"
      },
      "source": [
        "This section of code is similar to the 2 previous sections of code. First it fits the LDA models with the proper tf features. Then it prints all the relevant words from the 10 topics according to the LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQhnjZVam4nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Fitting LDA models with tf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}